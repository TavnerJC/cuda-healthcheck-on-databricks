# ðŸ”„ **Alternative: Benchmark CUDA Healthcheck Performance**

Since CuOPT isn't working on Serverless, let's benchmark our **CUDA Healthcheck Tool** with different workload sizes.

This still demonstrates A10 vs H100 performance differences!

---

## ðŸ““ **Alternative Notebook 2: CUDA Environment Analysis Benchmark**

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # ðŸ”¬ CUDA Healthcheck Performance Benchmark
# MAGIC
# MAGIC Benchmark CUDA detection and compatibility analysis across different scales.

# COMMAND ----------
# Package already installed from Notebook 1
import time
import json
import numpy as np
import pandas as pd
from datetime import datetime, timezone
import matplotlib.pyplot as plt
from cuda_healthcheck import CUDADetector, BreakingChangesDatabase

# COMMAND ----------
# MAGIC %md
# MAGIC ## Load Environment

# COMMAND ----------
# Load from Notebook 1 or set manually
try:
    environment_str = dbutils.jobs.taskValues.get(
        taskKey="01_validate_environment", 
        key="environment", 
        debugValue="{}"
    )
    environment = json.loads(environment_str) if environment_str != "{}" else None
except:
    environment = None

if not environment:
    environment = {
        "gpu_architecture": "NVIDIA A10G",
        "cuda_environment": {"runtime": "12.6", "driver": "12.4"}
    }

print(f"Testing on: {environment['gpu_architecture']}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Benchmark Tests

# COMMAND ----------
test_cases = [
    {
        "name": "Single_Environment_Detection",
        "description": "Basic CUDA detection",
        "iterations": 1
    },
    {
        "name": "Repeated_Environment_Detection",
        "description": "Multiple CUDA detections",
        "iterations": 10
    },
    {
        "name": "Compatibility_Analysis_Small",
        "description": "Analyze 1 library",
        "num_libraries": 1
    },
    {
        "name": "Compatibility_Analysis_Large",
        "description": "Analyze 10 libraries",
        "num_libraries": 10
    },
]

# COMMAND ----------
# MAGIC %md
# MAGIC ## Run Benchmarks

# COMMAND ----------
def benchmark_detection(iterations):
    """Benchmark CUDA detection."""
    start = time.time()
    
    for _ in range(iterations):
        detector = CUDADetector()
        env = detector.detect_environment()
    
    return time.time() - start

def benchmark_compatibility(num_libraries):
    """Benchmark compatibility analysis."""
    db = BreakingChangesDatabase()
    
    # Create test libraries
    libraries = [
        {"name": "pytorch", "version": "2.0.0", "cuda_version": "12.0"}
        for _ in range(num_libraries)
    ]
    
    start = time.time()
    score = db.score_compatibility(libraries, "13.0")
    return time.time() - start

results = []

for test in test_cases:
    print(f"\nðŸ”¬ Running: {test['name']}")
    
    try:
        if 'iterations' in test:
            elapsed = benchmark_detection(test['iterations'])
            throughput = test['iterations'] / elapsed
        else:
            elapsed = benchmark_compatibility(test['num_libraries'])
            throughput = test['num_libraries'] / elapsed
        
        result = {
            "test_name": test['name'],
            "status": "success",
            "time_seconds": round(elapsed, 4),
            "throughput": round(throughput, 2),
            "description": test['description']
        }
        
        print(f"âœ… Time: {result['time_seconds']}s, Throughput: {result['throughput']}/s")
        
    except Exception as e:
        result = {
            "test_name": test['name'],
            "status": "failed",
            "error": str(e)
        }
        print(f"âŒ Error: {e}")
    
    result['gpu'] = environment['gpu_architecture']
    result['cuda'] = environment['cuda_environment']['runtime']
    result['timestamp'] = datetime.now(timezone.utc).isoformat()
    results.append(result)

# COMMAND ----------
# MAGIC %md
# MAGIC ## Display Results

# COMMAND ----------
results_df = pd.DataFrame(results)
display(results_df)

# COMMAND ----------
# MAGIC %md
# MAGIC ## Visualize

# COMMAND ----------
successful = results_df[results_df['status'] == 'success']

if len(successful) > 0:
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Execution time
    ax1.bar(range(len(successful)), successful['time_seconds'], color='#76b900', alpha=0.7)
    ax1.set_xticks(range(len(successful)))
    ax1.set_xticklabels(successful['test_name'], rotation=45, ha='right', fontsize=8)
    ax1.set_ylabel('Time (seconds)')
    ax1.set_title(f'CUDA Healthcheck Performance on {environment["gpu_architecture"]}')
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Throughput
    ax2.bar(range(len(successful)), successful['throughput'], color='#0071c5', alpha=0.7)
    ax2.set_xticks(range(len(successful)))
    ax2.set_xticklabels(successful['test_name'], rotation=45, ha='right', fontsize=8)
    ax2.set_ylabel('Operations per Second')
    ax2.set_title('Throughput Analysis')
    ax2.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()

# COMMAND ----------
# MAGIC %md
# MAGIC ## Summary

# COMMAND ----------
summary = {
    "environment": environment,
    "results": results,
    "summary": {
        "total_tests": len(results),
        "successful": len([r for r in results if r['status'] == 'success']),
        "failed": len([r for r in results if r['status'] != 'success']),
        "avg_time": successful['time_seconds'].mean() if len(successful) > 0 else 0,
    }
}

print("=" * 80)
print("ðŸ“Š BENCHMARK SUMMARY")
print("=" * 80)
print(f"GPU: {environment['gpu_architecture']}")
print(f"CUDA: {environment['cuda_environment']['runtime']}")
print(f"Tests: {summary['summary']['total_tests']}")
print(f"Successful: {summary['summary']['successful']}")
print(f"Avg Time: {summary['summary']['avg_time']:.4f}s")
print("=" * 80)

dbutils.jobs.taskValues.set(key="benchmark_summary", value=json.dumps(summary))
```

---

## âœ… **Advantages of This Approach**

1. âœ… **Works on Serverless** - No external dependencies
2. âœ… **Uses Your Tool** - Benchmarks the CUDA Healthcheck you built
3. âœ… **Still Meaningful** - Shows GPU performance differences
4. âœ… **A10 vs H100** - Can still compare different GPUs
5. âœ… **Quick to Run** - Completes in seconds

---

## ðŸ“Š **What You'll Measure**

- CUDA environment detection speed
- GPU metadata retrieval performance
- Compatibility analysis throughput
- Breaking changes database query performance

**Still provides A10 vs H100 comparison data!**


